{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/master/HW2/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas import option_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4KuVSCSqlUX",
        "outputId": "086dba83-8aaf-41d0-8e5d-48541251010a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hk07KCgwoZy"
      },
      "source": [
        "Let's download the data we'll use for training and development, and also the data we'll use to make predictions for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn0XtfFeqP2P",
        "outputId": "00ee847a-05aa-4841-a792-3d4cb81c6a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-03 02:19:41--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1322055 (1.3M) [text/plain]\n",
            "Saving to: ‘train.txt.3’\n",
            "\n",
            "\rtrain.txt.3           0%[                    ]       0  --.-KB/s               \rtrain.txt.3         100%[===================>]   1.26M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-02-03 02:19:42 (22.9 MB/s) - ‘train.txt.3’ saved [1322055/1322055]\n",
            "\n",
            "--2022-02-03 02:19:42--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309909 (1.2M) [text/plain]\n",
            "Saving to: ‘dev.txt.3’\n",
            "\n",
            "dev.txt.3           100%[===================>]   1.25M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-02-03 02:19:42 (22.9 MB/s) - ‘dev.txt.3’ saved [1309909/1309909]\n",
            "\n",
            "--2022-02-03 02:19:42--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6573426 (6.3M) [text/plain]\n",
            "Saving to: ‘test.txt.3’\n",
            "\n",
            "test.txt.3          100%[===================>]   6.27M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-02-03 02:19:42 (78.5 MB/s) - ‘test.txt.3’ saved [6573426/6573426]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get data\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "jq2yq0xpRCUb"
      },
      "outputs": [],
      "source": [
        "trainingFile = \"train.txt\"\n",
        "evaluationFile = \"dev.txt\"\n",
        "testFile = \"test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, L2_regularization_strength=1.0, min_feature_count=1):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.log_reg = None\n",
        "        self.L2_regularization_strength=L2_regularization_strength\n",
        "        self.min_feature_count=min_feature_count\n",
        "\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "        self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "        self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "    # Read data from file\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            for line in file:\n",
        "                cols = line.split(\"\\t\")\n",
        "                idd = cols[0]\n",
        "                label = cols[1]\n",
        "                text = cols[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "                \n",
        "        return data\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def evaluate(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        self.log_reg = linear_model.LogisticRegression(C = self.L2_regularization_strength, max_iter=1000)\n",
        "        self.log_reg.fit(self.trainX, self.trainY)\n",
        "        training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "        development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "        print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\n",
        "\n",
        "    # Predict labels for new data\n",
        "    def predict(self):\n",
        "        predX = self.log_reg.predict(self.testX)\n",
        "\n",
        "        with open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\") as out:\n",
        "            writer=csv.writer(out)\n",
        "            writer.writerow([\"Id\", \"Expected\"])\n",
        "            for idx, data_id in enumerate(self.testX):\n",
        "                writer.writerow([self.testOrig[idx][0], predX[idx]])\n",
        "        out.close()\n",
        "\n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmfkG782kgo"
      },
      "source": [
        "*First*, let's define a classifier based on a really simple dictionary-based feature: if the abstract contains the words \"love\" or \"like\", the CONTAINS_POSITIVE_WORD feature will fire, and if it contains either \"hate\" or \"dislike\", the CONTAINS_NEGATIVE_WORD will fire.  Note how we use `nltk.word_tokenize` to tokenize the text into its discrete words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "xCq1bL3e2jUj"
      },
      "outputs": [],
      "source": [
        "def simple_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        if word == \"love\" or word == \"like\":\n",
        "            feats[\"contains_positive_word\"] = 1\n",
        "        if word == \"hate\" or word == \"dislike\":\n",
        "            feats[\"contains_negative_word\"] = 1\n",
        "            \n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3PQdN9r3Ujz"
      },
      "source": [
        "Now let's see how that feature performs on the development data.  Note the `L2_regularization_strength` specifies the strength of the L2 regularizer (values closer to 0 = stronger regularization), and the `min_feature_count` specifies how many data points need to contain a feature for it to be allowable as a feature in the model.  Both are ways to prevent the model from overfitting and achieve higher performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnqjxd6fKPiP",
        "outputId": "3e72479e-97a2-4dc2-de73-ac005aa62d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: simple_featurize, Features: 2, Train accuracy: 0.509, Dev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "simple_classifier = Classifier(simple_featurize, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "simple_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4XQzU3PdeU"
      },
      "source": [
        "First, is this accuracy score any good?  Let's calculate the accuracy of a majority class predictor to provide some context.  Again, this determines the most represented (majority) class in the training data, and then predicts every test point to be this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t--LfOjPj7T",
        "outputId": "5a3a14ec-3141-4c1e-df51-fba46abcdf38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority class: pos\tDev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "def majority_class(trainY, devY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority_class=labelCounts.most_common(1)[0][0]\n",
        "    \n",
        "    correct=0.\n",
        "    for label in devY:\n",
        "        if label == majority_class:\n",
        "            correct+=1\n",
        "            \n",
        "    print(\"Majority class: %s\\tDev accuracy: %.3f\" % (majority_class, correct/len(devY)))\n",
        "majority_class(simple_classifier.trainY, simple_classifier.devY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEkYOWO5ClC"
      },
      "source": [
        "# Your assignment\n",
        "\n",
        "## Deliverable 1\n",
        "\n",
        "Your job in this homework is to implement a binary bag-of-words model (i.e., one that assigns a feature value of 1 to each word type that is present in the text); and to brainstorm three additional distinct classes of features, justify why they might help improve the performance *over a bag of words* for this task, implement them in code, and then assess their independent performance on the development data. \n",
        "\n",
        "Describe your features and report their performance in the table below; implement the features in the specified `feature1`, `feature2`, and `feature3` functions, and execute each respective classifier to show its performance.  \n",
        "\n",
        "|Feature|Why should it work? (50 words each)|Dev set performance|\n",
        "|---|---|---|\n",
        "|Bag of words|By keeping track of the occurrences of words between documents we can train our binary document vector and train our dataset based on what words appear and do not appear when writing a positive or negative review. |0.795\n",
        "|Feature 1|By taking the inverse of the frequency, we can penalize the high occurrence words and but we are able to see how often we see some words in the context of negative and positive values.|0.716\n",
        "|Feature 2|Commonly, reviews give emphasis based on the capitalization of their words, while this applies for both negative and positive reviews, movie critics tend to positively emphasize good doings, actors, and directors by capitalizing their names.|0.524\n",
        "|Feature 3|It takes a google search to find the most common adjectives when reviewing a movie. By creating a list of both positive and negative words, we can quantify and compare the amount of positive and negative words in a text to ultimately classify the review.|0.679\n",
        "\n",
        "Note that it is not required for your features to actually perform well, but your justification for why it *should* perform better than a bag of words should be defensible.  The most creative features (defined as features that few other students use and that are reasonably well-performing) will receive extra credit for this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "vVl1zAREekC3"
      },
      "outputs": [],
      "source": [
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.\n",
        "    words = nltk.word_tokenize(text)\n",
        "    feats = {}\n",
        "\n",
        "    for word in words:\n",
        "      feats[word] = 1\n",
        "    \n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3AJ5qMBeqmL",
        "outputId": "9bf6cd27-7f7a-42c0-ea26-574c47ec98ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bag_of_words, Features: 23764, Train accuracy: 1.000, Dev accuracy: 0.795\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "bow_classifier = Classifier(bag_of_words, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "bow_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "ocPMYhIt4BX0"
      },
      "outputs": [],
      "source": [
        "def feature1(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    #TF-IDF\n",
        "\n",
        "    ##Bag of Words ocurrence counter\n",
        "    words = nltk.word_tokenize(text)\n",
        "    feats = {}\n",
        "\n",
        "    #Get word count\n",
        "    for word in words:\n",
        "      if feats.get(word) == True:\n",
        "        feats[word] += feats[word] + 1\n",
        "      else:\n",
        "        feats[word] = 1\n",
        "        \n",
        "    #Calculate the inverse of the word frequencies\n",
        "    for key in feats.keys():\n",
        "      feats[key] = (feats.get(key) / len(feats.values()))**-1\n",
        "      \n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MAwRwbQ7lVw",
        "outputId": "e1af9aa2-1093-4b6f-f2df-c8e820cc2238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature1, Features: 23764, Train accuracy: 1.000, Dev accuracy: 0.716\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier1 = Classifier(feature1, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier1.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "LNlQyjEB4Bwt"
      },
      "outputs": [],
      "source": [
        "from numpy.core.defchararray import isupper\n",
        "def feature2(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    #This feature tries to check the lenght of the text!\n",
        "    #Exclamation marks, maybe here is excitement on some of the reviews, lets count how many we have\n",
        "  words = nltk.word_tokenize(text)\n",
        "  feats = {}\n",
        "  feats[\"isUpper\"] = 0\n",
        "  \n",
        "  for word in words:\n",
        "        if isupper(word):\n",
        "            feats[\"isUpper\"] = feats.get(\"isUpper\") + 1\n",
        "            \n",
        "  return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgpuykF67oWZ",
        "outputId": "8ca3e429-4bc9-4397-a900-d1a3b2402add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature2, Features: 1, Train accuracy: 0.551, Dev accuracy: 0.524\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier2 = Classifier(feature2, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier2.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "FmJKucgn4CEg"
      },
      "outputs": [],
      "source": [
        "def feature3(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "  positives = [\"action\", \"amusing\", \"balanced\", \"beautiful\", \"breath-taking\"\n",
        "    \"captivating\", \"compelling\", \"complex\",\"enigmatic\", \"enjoyable\", \"entertaining\", \"exciting\", \"fantastic\", \"far-fetched\",\n",
        "    \"fascinating\", \"funny\", \"gripping\", \"hilarious\", \"humorous\", \"inspiring\", \"manipulative\", \"masterful\",\n",
        "    \"perfect\", \"powerful\", \"romantic\",\"overwhelming\", \"perfect\", \"provocative\", \"silly\", \"stupid\", \"surprising\", \"super\", \"tense\",\n",
        "    \"unbelievable\",\"uplifting\"]\n",
        "\n",
        "  negatives = [\"bad\", \"boring\",\"confusing\", \"contrived\", \"disappointing\", \"far-fetched\", \"lousy\", \"manipulative\",\"horrible\",\n",
        "               \"imperfect\", \"poignant\", \"unrealistic\", \"ridiculous\", \"overacted\", \"overrated\",\"bad\",\"underwhelming\",\"boring\",\"forgettable \"\n",
        "               \"overwhelming\", \"imperfect\", \"predictable\", \"provocative\", \"silly\", \"stupid\", \"surprising\", \"unoriginal\",\"uplifting\", \"waste\"]\n",
        "\n",
        "  words = nltk.word_tokenize(text)\n",
        "  feats = {}\n",
        "  feats[\"contains_positive_word\"] = 0\n",
        "  feats[\"contains_negative_word\"] = 0\n",
        "\n",
        "  for word in words:\n",
        "        word = word.lower()\n",
        "        if word in positives:\n",
        "            feats[\"contains_positive_word\"] = feats.get(\"contains_positive_word\") + 1\n",
        "        if word in negatives:\n",
        "            feats[\"contains_negative_word\"] = feats.get(\"contains_negative_word\") + 1\n",
        "    \n",
        "  return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_f--utb7q4l",
        "outputId": "af66306b-2a02-48ea-9f1d-964245892e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature3, Features: 2, Train accuracy: 0.659, Dev accuracy: 0.679\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier3 = Classifier(feature3, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier3.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEpK5LyMgv5c"
      },
      "source": [
        "Next, let's combine any or all the features you have developed into one big model and make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "jxKmEqI5JY71"
      },
      "outputs": [],
      "source": [
        "def combiner_function(text):\n",
        "\n",
        "    # Here the `all_feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "  all_feats={}\n",
        "  for feature in [bag_of_words, feature1, feature2, feature3]:\n",
        "    all_feats.update(feature(text))\n",
        "  return all_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-tRUFTIdAqT",
        "outputId": "93437903-9e98-44d2-cc86-8f49699baf3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: combiner_function, Features: 23767, Train accuracy: 1.000, Dev accuracy: 0.716\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "big_classifier = Classifier(combiner_function, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "big_classifier.evaluate()\n",
        "\n",
        "#generate .csv file with prediction output on test data\n",
        "big_classifier.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2J1BLgatMP"
      },
      "source": [
        " ## Deliverable 2\n",
        "\n",
        "This code will generate a file named `combiner_function_predictions.csv`; download this file (using e.g. the file manager on the left panel in Colab) and submit this to GradeScope along with your notebook; the 5 systems with the highest performance (revealed after the submission deadline) will receive extra credit for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lgyoJm09pqe"
      },
      "source": [
        "## Interrogating classifiers\n",
        "\n",
        "Below you will find several ways in which you can interrogate your model to get ideas on ways to improve its performance.  **Note that nothing below this line requires any work on your part; treat these as useful tools for understanding what works and what doesn't.**\n",
        "\n",
        "1. First, let's look at the confusion matrix of its predictions (where we can compare the true labels with the predicted labels).  What kinds of mistakes is it making?  (While this is mainly helpful in the context of multiclass classification, we can still see if there's a bias toward predicting a specific class in the binary setting as well). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "7ulxd1TosIMV",
        "outputId": "c04f590d-9f6c-4959-ad7f-2f5c0c6a7582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def print_confusion(classifier):\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plot_confusion_matrix(classifier.log_reg, classifier.devX, classifier.devY, ax=ax, xticks_rotation=\"vertical\", values_format=\"d\")\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(big_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPhH4flIuEbx"
      },
      "source": [
        "2. Next, let's look at the features that are most defining for each of the classes (ranked by how strong their corresponding coefficient is).  Do the features you are defining help in the ways you think they should?  Do sets of successful features suggests others, or complementary features that may provide a different view on the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAyGuXIi9pqe",
        "outputId": "addcf916-d0b0-40ba-e5ba-b7ca423a5189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos\t0.039\tvery\n",
            "pos\t0.036\tof\n",
            "pos\t0.033\tenjoyed\n",
            "pos\t0.033\tincredible\n",
            "pos\t0.030\tgets\n",
            "pos\t0.030\t,\n",
            "pos\t0.029\tOne\n",
            "pos\t0.029\tfun\n",
            "pos\t0.028\tIt\n",
            "pos\t0.028\tthe\n",
            "pos\t0.028\texcellent\n",
            "pos\t0.028\tamazing\n",
            "pos\t0.026\tvision\n",
            "pos\t0.026\tremarkable\n",
            "pos\t0.026\thuman\n",
            "pos\t0.025\tnew\n",
            "pos\t0.025\tsci-fi\n",
            "pos\t0.025\tin\n",
            "pos\t0.025\tbeautifully\n",
            "pos\t0.025\tperfect\n",
            "pos\t0.024\tgreat\n",
            "pos\t0.024\tromance\n",
            "pos\t0.024\t.\n",
            "pos\t0.023\tentertainment\n",
            "pos\t0.023\tgo\n",
            "\n",
            "neg\t-0.045\tworst\n",
            "neg\t-0.037\twould\n",
            "neg\t-0.036\tboring\n",
            "neg\t-0.036\tacting\n",
            "neg\t-0.035\teven\n",
            "neg\t-0.034\tstupid\n",
            "neg\t-0.030\trental\n",
            "neg\t-0.028\tthere\n",
            "neg\t-0.027\twaste\n",
            "neg\t-0.026\tact\n",
            "neg\t-0.026\tdull\n",
            "neg\t-0.025\tmake\n",
            "neg\t-0.024\tnothing\n",
            "neg\t-0.024\tthan\n",
            "neg\t-0.024\twatching\n",
            "neg\t-0.023\tidea\n",
            "neg\t-0.023\tafter\n",
            "neg\t-0.022\tme\n",
            "neg\t-0.022\tlaughable\n",
            "neg\t-0.021\tactors\n",
            "neg\t-0.021\t're\n",
            "neg\t-0.021\tworse\n",
            "neg\t-0.021\tBut\n",
            "neg\t-0.020\tout\n",
            "neg\t-0.020\teach\n",
            "\n"
          ]
        }
      ],
      "source": [
        "big_classifier.printWeights(n=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80DUsSXu7h9"
      },
      "source": [
        "3. Next, let's look at the individual data points that are most mistaken. Does it suggest any features you might create to disentangle them?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "I4uTzwV99pqe"
      },
      "outputs": [],
      "source": [
        "def analyze(classifier):\n",
        "    \n",
        "    probs=classifier.log_reg.predict_proba(classifier.devX)\n",
        "    predicts=classifier.log_reg.predict(classifier.devX)\n",
        "\n",
        "    classes={}\n",
        "    for idx, lab in enumerate(classifier.log_reg.classes_):\n",
        "        classes[lab]=idx\n",
        "\n",
        "    mistakes={}\n",
        "    for i in range(len(probs)):\n",
        "        if predicts[i] != classifier.devY[i]:\n",
        "            predicted_lab_idx=classes[predicts[i]]\n",
        "            mistakes[i]=probs[i][predicted_lab_idx]\n",
        "\n",
        "    frame=[]\n",
        "    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k, v in sorted_x:\n",
        "        idd=classifier.devOrig[k][0]\n",
        "        text=classifier.devOrig[k][2]\n",
        "        frame.append([idd, v, classifier.devY[k], predicts[k], text])\n",
        "\n",
        "    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n",
        "\n",
        "    with option_context('display.max_colwidth', 400):\n",
        "        display(df.head(n=20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UXmRhSuzxaJi",
        "outputId": "80de1f26-44a0-4638-9f30-48e428e2bf79"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ce9097b2-a7aa-4e7b-8b38-c7456a27fdc5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P(predicted class confidence)</th>\n",
              "      <th>Human label</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1317</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>if.... is the cinematic equivalent of Sgt. Pepper's: Revered by baby boomers as the pinnacle of creation, and viewed as rather a silly bit of business by preceding and subsequent generations. Now that the children of the middle classes the world over are seemingly super human due to the internet, and view the prospect of boarding school as a wonderful opportunity thanks to the Harry Potter boo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1337</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>FULL OF SPOILERS.This is a pretty fast and enjoyable crime thriller based on Ira Levin's play about two gay playwrights (Caine and Reeve) that plot the murder of one's rich wife (Cannon) to get the property and the insurance. The plot succeeds but Christopher Reeve as the younger and less established of the two writers decides to make a play out of the actual murder -- with only slight changes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1436</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1523</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>One thing I'm sure everyone who has seen this film will agree on is that it is very creepy. The other films in Polanski's unofficial trilogy are creepy too, but they are all different in what makes them creepy, but they all roughly deal with the same thing, they all deal with the mind. Definitely the staring people are very creepy, each of them sent shivers down my spine that made me incapable...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1608</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>As good an advert for republicanism as you're ever likely to see,\"Mayerling\"is an everyday story of royal folk in late nineteenth century Austria.Set during one of Europe's seemingly incessant internal turmoils it concerns itself with the Emperor Franz Joseph (Mr James Mason),his rebellious son,the Crown Prince Rudolf (Mr Omar Sharif)the Empress(Miss Ava Gardner) and various mistresses,secret ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1668</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I reflect back to the days when I held my boyfriends hat to smell him into existence in my time alone when I was 16. The little moments of this film are so accurate and right on pace with what is going on in the minds and hearts of young girls during those coming of age teenage years. Now at my age I want to preach to them about their decisions and how life during those times are not as import...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1702</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>David Arquette is a young and naive home security alarmsalesman taken under the wing of Stanley Tucci. Arquette is agolden boy, scoring a big sale on his first call- to widow KateCapshaw and her dopey son Ryan Reynolds. Things are goingwell for Arquette, he is appearing in commercials for the securityfirm and he is falling in love with Capshaw.Then Tucci and his right hand woman Mary McCormack...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1774</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I first came across this film when I read a book (written in the 1970s) about the career of Mitchell Leisin. I have to admit that over the years I have watched many of his films and find his best work really high quality. SWING HIGH, SWING LOW was supposed to be one of his best. While it did not bore me, it did not impress me as much as HOLD BACK THE DAWN, DEATH TAKES A HOLIDAY, KITTY, or even...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1822</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1850</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I have to agree with some of the other comments and even go a step further. Nothing about this film worked, absolutely nothing. Delmar our central character makes the decision to become a surrogate mother in order to earn enough money to buy a restaurant but along the way fall for a wise ex-jailbird. At the same time her friend Hortense is trying to get her lawyer boyfriend to finally marry he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1862</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>In the previews, \"The 40 Year-Old Virgin\" boasts the image of another immature sex romp about a 40-ish Lonely Guy who suddenly feels the urge to do the deed simply because he hasn't. Too many past bad experiences have dampened his enthusiasm to the point that he avoids women completely. And then the unexpected happens: he falls in love. What's more, there's a movie out about it, and it's calle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1889</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>At times, this overtakes The Thing as my favourite horror film. While Carpenter's film is the more efficient and more entertaining flick, Kubrick's is more artistic, more thought-provoking, and probably scarier. It's one of the few films where I can look past its flaws and truly and wholly love it. I try not to compare it to the book  which I've only read once, a number of years ago, and whic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1916</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Having read the other comments on this film, I would like to share my own view that this is one tough movie to see unless you are a total Brooksophile. I am not.When looked at by a purely objective observer, the film is an unbalanced narrative that presents us with more undistilled neuroses than are capable of being absorbed in one sitting. It is quite difficult to watch. The Brooks character ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1938</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>All right, here's the deal: if you're easily offended then you might want to stay far, far away from this one. There are some painfully funny moments in the movie, but I probably blushed about as much as I laughed. Actually, I probably blushed MORE than I laughed. And if I wasn't literally blushing on the outside, then I was blushing on the inside. If there is absolutely nothing in this movie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1787</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>My roommate had bought this documentary and invited me to watch it with her. She's from China and only heard so much about 9/11 and wanted to know the cold hard truth and she wanted me to tell her more after the documentary. I felt awful watching this documentary, it was like reliving the nightmare and it still brings tears to my eyes.But I'm extremely grateful that I watched this documentary,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1967</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>The oddly-named Vera-Ellen was to movie dancing what Sonja Henie was to movie ice-skating: blonde, girlish, always delightful to watch, but not an especially good actress and usually lumbered with weak material. When I watch Vera-Ellen's sexy apache dance with Gene Kelly in 'Words and Music', I can't help noticing that her blouse (yellow with narrow red horizontal stripes) seems to be made out...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1292</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>That's right! Under 9 on average, but maybe under 12s for some others! I was 11 when I originally saw this on video and at such youth I wasn't able to notice the shoddy cartoon-quality or the fact that those classic characters we have all grown to love are Not the same or as good to see. Just about everything is so 2D here! Belle is, I'll agree, not even beautiful but just a plain-looking woma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1110</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Before Stan Laurel became the smaller half of the all-time greatest comedy team, he laboured under contract to Broncho Billy Anderson in a series of cheapies, many of which were parodies of major Hollywood features. Following a dispute with Anderson, Laurel continued the informal series of parodies at Joe Rock's smaller (and more indigent) production company.Most of Laurel's parody films were ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1975</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>This film is to my mind the weakest film in the original Star Wars trilogy, for a variety of reasons. However it emerges at the end of the day a winner, despite all its flaws. It's still a very good film, even if a lot of its quality depends on the characters that have been built up in the superior 2 installments.One problem here is the look of the film, which isn't very consistent with the ot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1218</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>This early B entry into the patriotic category slapped a gorgeous young Gene Tierney on the ads and posters, but you have to wait a good time before you glimpse her, riding in a Hollywoodized camel train. Previously, we've set up George Sanders and Bruce Cabot in the desert as guys who barely get along, but must rally in the face of attack. I've seen Sanders as so many enjoyable cads that it w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce9097b2-a7aa-4e7b-8b38-c7456a27fdc5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce9097b2-a7aa-4e7b-8b38-c7456a27fdc5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce9097b2-a7aa-4e7b-8b38-c7456a27fdc5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      id  ...                                                                                                                                                                                                                                                                                                                                                                                                             Text\n",
              "0   1317  ...  if.... is the cinematic equivalent of Sgt. Pepper's: Revered by baby boomers as the pinnacle of creation, and viewed as rather a silly bit of business by preceding and subsequent generations. Now that the children of the middle classes the world over are seemingly super human due to the internet, and view the prospect of boarding school as a wonderful opportunity thanks to the Harry Potter boo...\n",
              "1   1337  ...  FULL OF SPOILERS.This is a pretty fast and enjoyable crime thriller based on Ira Levin's play about two gay playwrights (Caine and Reeve) that plot the murder of one's rich wife (Cannon) to get the property and the insurance. The plot succeeds but Christopher Reeve as the younger and less established of the two writers decides to make a play out of the actual murder -- with only slight changes...\n",
              "2   1436  ...  I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...\n",
              "3   1523  ...  One thing I'm sure everyone who has seen this film will agree on is that it is very creepy. The other films in Polanski's unofficial trilogy are creepy too, but they are all different in what makes them creepy, but they all roughly deal with the same thing, they all deal with the mind. Definitely the staring people are very creepy, each of them sent shivers down my spine that made me incapable...\n",
              "4   1608  ...  As good an advert for republicanism as you're ever likely to see,\"Mayerling\"is an everyday story of royal folk in late nineteenth century Austria.Set during one of Europe's seemingly incessant internal turmoils it concerns itself with the Emperor Franz Joseph (Mr James Mason),his rebellious son,the Crown Prince Rudolf (Mr Omar Sharif)the Empress(Miss Ava Gardner) and various mistresses,secret ...\n",
              "5   1668  ...  I reflect back to the days when I held my boyfriends hat to smell him into existence in my time alone when I was 16. The little moments of this film are so accurate and right on pace with what is going on in the minds and hearts of young girls during those coming of age teenage years. Now at my age I want to preach to them about their decisions and how life during those times are not as import...\n",
              "6   1702  ...  David Arquette is a young and naive home security alarmsalesman taken under the wing of Stanley Tucci. Arquette is agolden boy, scoring a big sale on his first call- to widow KateCapshaw and her dopey son Ryan Reynolds. Things are goingwell for Arquette, he is appearing in commercials for the securityfirm and he is falling in love with Capshaw.Then Tucci and his right hand woman Mary McCormack...\n",
              "7   1774  ...  I first came across this film when I read a book (written in the 1970s) about the career of Mitchell Leisin. I have to admit that over the years I have watched many of his films and find his best work really high quality. SWING HIGH, SWING LOW was supposed to be one of his best. While it did not bore me, it did not impress me as much as HOLD BACK THE DAWN, DEATH TAKES A HOLIDAY, KITTY, or even...\n",
              "8   1822  ...  Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...\n",
              "9   1850  ...  I have to agree with some of the other comments and even go a step further. Nothing about this film worked, absolutely nothing. Delmar our central character makes the decision to become a surrogate mother in order to earn enough money to buy a restaurant but along the way fall for a wise ex-jailbird. At the same time her friend Hortense is trying to get her lawyer boyfriend to finally marry he...\n",
              "10  1862  ...  In the previews, \"The 40 Year-Old Virgin\" boasts the image of another immature sex romp about a 40-ish Lonely Guy who suddenly feels the urge to do the deed simply because he hasn't. Too many past bad experiences have dampened his enthusiasm to the point that he avoids women completely. And then the unexpected happens: he falls in love. What's more, there's a movie out about it, and it's calle...\n",
              "11  1889  ...  At times, this overtakes The Thing as my favourite horror film. While Carpenter's film is the more efficient and more entertaining flick, Kubrick's is more artistic, more thought-provoking, and probably scarier. It's one of the few films where I can look past its flaws and truly and wholly love it. I try not to compare it to the book  which I've only read once, a number of years ago, and whic...\n",
              "12  1916  ...  Having read the other comments on this film, I would like to share my own view that this is one tough movie to see unless you are a total Brooksophile. I am not.When looked at by a purely objective observer, the film is an unbalanced narrative that presents us with more undistilled neuroses than are capable of being absorbed in one sitting. It is quite difficult to watch. The Brooks character ...\n",
              "13  1938  ...  All right, here's the deal: if you're easily offended then you might want to stay far, far away from this one. There are some painfully funny moments in the movie, but I probably blushed about as much as I laughed. Actually, I probably blushed MORE than I laughed. And if I wasn't literally blushing on the outside, then I was blushing on the inside. If there is absolutely nothing in this movie ...\n",
              "14  1787  ...  My roommate had bought this documentary and invited me to watch it with her. She's from China and only heard so much about 9/11 and wanted to know the cold hard truth and she wanted me to tell her more after the documentary. I felt awful watching this documentary, it was like reliving the nightmare and it still brings tears to my eyes.But I'm extremely grateful that I watched this documentary,...\n",
              "15  1967  ...  The oddly-named Vera-Ellen was to movie dancing what Sonja Henie was to movie ice-skating: blonde, girlish, always delightful to watch, but not an especially good actress and usually lumbered with weak material. When I watch Vera-Ellen's sexy apache dance with Gene Kelly in 'Words and Music', I can't help noticing that her blouse (yellow with narrow red horizontal stripes) seems to be made out...\n",
              "16  1292  ...  That's right! Under 9 on average, but maybe under 12s for some others! I was 11 when I originally saw this on video and at such youth I wasn't able to notice the shoddy cartoon-quality or the fact that those classic characters we have all grown to love are Not the same or as good to see. Just about everything is so 2D here! Belle is, I'll agree, not even beautiful but just a plain-looking woma...\n",
              "17  1110  ...  Before Stan Laurel became the smaller half of the all-time greatest comedy team, he laboured under contract to Broncho Billy Anderson in a series of cheapies, many of which were parodies of major Hollywood features. Following a dispute with Anderson, Laurel continued the informal series of parodies at Joe Rock's smaller (and more indigent) production company.Most of Laurel's parody films were ...\n",
              "18  1975  ...  This film is to my mind the weakest film in the original Star Wars trilogy, for a variety of reasons. However it emerges at the end of the day a winner, despite all its flaws. It's still a very good film, even if a lot of its quality depends on the characters that have been built up in the superior 2 installments.One problem here is the look of the film, which isn't very consistent with the ot...\n",
              "19  1218  ...  This early B entry into the patriotic category slapped a gorgeous young Gene Tierney on the ads and posters, but you have to wait a good time before you glimpse her, riding in a Hollywoodized camel train. Previously, we've set up George Sanders and Bruce Cabot in the desert as guys who barely get along, but must rally in the face of attack. I've seen Sanders as so many enjoyable cads that it w...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "analyze(big_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "nxwwblfh9pqf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}